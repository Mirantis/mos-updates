
## Table of Contents
 - [Inventory](#inventory)
 - [Folder structure](#folder-structure)
   - [Fuel](#fuel)
   - [Nodes](#nodes)
 - [Playbooks](#playbooks)
   - [gather_customizations.yml](#gather_customizationsyml)
   - [verify_patches.yml](#verify_patchesyml)
   - [update_fuel.yml](#update_fuelyml)
   - [apply_mu.yml](#apply_muyml)
   - [restart_services.yml](#restart_servicesyml)
   - [rollback.yml](#rollbackyml)
   - [update_ceph.yml](#update_cephyml)
 - [Tasks](#tasks)
   - [health_checks.yml](health_checksyml)
   - [apt_update.yml](#apt_updateyml)
   - [apt_update_action.yml](#apt_update_actionyml)
   - [verify_md5.yml](#verify_md5yml)
   - [get_current_mu.yml](get_current_muyml)
   - [gather_customizations.yml](#gather_customizationsyml)
   - [verify_customizations.yml](#verify_customizationsyml)
   - [verify_patches.yml](#verify_patchesyml)
   - [apt_upgrade.yml](#apt_upgradeyml)
   - [apply_patches.yml](#apply_patchesyml)
   - [rollback_upgrade.yml](#rollback_upgradeyml)
   - [update_ceph.yml](#tasksupdate_cephyml)
   - [restart_ceph.yml](#restart_cephyml)


Inventory
=========

Inventory Python script generates inventory data for Ansible using Fuel API.
For review inventory you can run this script separately.

[fuel_inventory.py](../inventory/fuel_inventory.py)

Folder structure
================

By default it looks like this (might be configured in conf file):

Fuel
----
Variables in config:
```
fuel_backup_dir:          "/root/fuel_mos_mu/fuel_backup"
fuel_dir:                 "/root/fuel_mos_mu/env_{{ env_id }}"
fuel_custom_dir:          "{{ fuel_dir }}/customizations"
fuel_custom_backup_dir:   "{{ fuel_dir }}/customizations_backup"
fuel_patches_dir:         "{{ fuel_dir }}/patches"
fuel_custom_verification: "{{ fuel_dir }}/customizations_verification"
fuel_unified_dir:         "{{ fuel_dir }}/customizations_unified"
```

Directory tree:
```
../fuel_mos_mu/
└── env_1
    ├── customizations
    │   ├── node-1
    │   │   ├── neutron-common_customization.patch
    │   │   └── nova-compute_customization.patch
    │   ├── node-2
    │   ├── node-3
    │   │   └── neutron-common_customization.patch
    │   ├── node-4
    │   └── node-5
    ├── customizations_backup
    │   └── customizations__09.27.16__03-39-12.tgz
    ├── customizations_unified
    │   ├── neutron-common_customization.patch
    │   └── nova-compute_customization.patch
    ├── customizations_verification
    │   ├── neutron-common
    │   │   ├── node-1
    │   │   │   └── neutron-common_customization.patch
    │   │   └── node-3
    │   │       └── neutron-common_customization.patch
    │   └── nova-compute
    │       └── node-1
    │           └── nova-compute_customization.patch
    └── patches
        ├── 00-customizations
        │   ├── neutron-common_customization.patch
        │   └── nova-compute_customization.patch
        └── 01-neutron.patch
```

* Each environment has its own folder **env_<env_id>**

* **customizations** is used for gathering customizations from nodes.
  Customizations are placed in folder with nodename.
  Customizations are gathered when this folder is absent or if you want to
  gather it from scratch please use flag **gather_customizations: true**.
  This folder is generated by [gather_customizations.yml](#gather_customizationsyml)

* **customizations_backup** contains backups of gathered customizations.
  Backup is generated after each gathering customizations process.

* **customizations_verifications** is used for processing of customizations
  which contains set of folders (package customization) across the environment.
  This folder is generated by [verify_customizations.yml](#verify_customizationsyml)

* **customizations_unified** contains unified customizations which will be
  copied to **patches/00-customizations** folder and used for applying after updating.

* **patches** is used for storing set of patches which will be synced
  on nodes and used for verifying and applying on cloud.
  Patches should have **.patch** extensions. Please be aware that patches
  will be applied in alphabetic order, also keep in mind that if flag
  **use_current_customizations: true** is enabled current customizations
  will be also copied to **patches** folder on nodes with to
  **00-customizations** folder.
  So it is recommended to name patches with prefixes like this
  **01-\<patchname\>.patch, 02-\<patchname2\>.patch**.

Nodes
-----
Variables in config:
```
mos_dir:          "/root/mos_mu"
custom_dir:       "{{ mos_dir }}/customizations"
patches_dir:      "{{ mos_dir }}/patches"
verification_dir: "{{ mos_dir }}/verification"
apt_dir:          "{{ mos_dir }}/apt"
apt_conf:         "{{ apt_dir }}/apt.conf"
apt_src_dir:      "{{ apt_dir }}/sources.list.d"
```

Directory tree:
```
/root/mos_mu/
├── apt
│   ├── apt.conf
│   ├── preferences.d
│   │   └── mos.pref
│   └── sources.list.d
│       ├── fuel.list
│       ├── GA.list
│       ├── mu-1.list
│       ├── mu-2.list
│       └── mu-3.list
├── customizations
│   ├── neutron-common
│   │   ├── 2%3a7.1.1-4~u14.04+mos82
│   │   │   └── ...
│   │   └── neutron-common_customization.patch
│   └── nova-compute
│       ├── 2%3a12.0.4-1~u14.04+mos10
│       │   └── ...
│       └── nova-compute_customization.patch
├── patches
│   ├── 00-customizations
│   │   ├── neutron-common_customization.patch
│   │   └── nova-compute_customization.patch
│   └── 01-neutron.patch
└── verification
    ├── neutron-common
    │   ├── 2%3a7.1.1-4~u14.04+mos82
    │   │   ├── neutron-common_2%3a7.1.1-4~u14.04+mos82_all.deb
    │   │   └── ...
    │   └── neutron-common_customization.patch
    ├── nova-compute
    │   ├── 2%3a12.0.4-1~u14.04+mos10
    │   │   ├── nova-compute_2%3a12.0.4-1~u14.04+mos10_all.deb
    │   │   └── ...
    │   └── nova-compute_customization.patch
    └── python-neutron
        ├── 01-neutron.patch
        └── 2%3a7.1.1-4~u14.04+mos82
            ├── python-neutron_2%3a7.1.1-4~u14.04+mos82_all.deb
            └── ...
```

* **apt** contains apt.conf which is always used for apt and uses
  only **sources.lists.d** folder for sources lists.

* **apt/sources.list.d** contains sources lists for all configured in config
  repositories.

* **apt/preferences.d** contains preferences for package pinning.

* **customizations** folder consists of folders for customized packages.
  Package folder contains a folder (current package version) with unpacked
  package and diff file between this unpacked (original) version and current
  installed (customized) version.

* **patches** folder contains all patches from Fuel **patches** folder and
  This folder is cleared every time when task [verify_patches.yml](#verify_patchesyml)
  is started.

* **verification** folder consists of folders for customized packages.
  Packages folder contains folder (candidate package version, by default,
  configured by flag **pkg_ver_for_verification: "Candidate"**) with unpacked
  package and patches files witch should be applied.

Playbooks
=========

By default all playbooks are defined for all nodes except Fuel.
It might be run for any node and group of nodes using standard flag **--limit**
like this `--limit=env_2:compute` (all computes in env_2).

All playbooks include variable file
[vars/mos_releases/{{ mos_release }}.yml](../playbooks/vars/mos_releases)
based on **mos_release** variable, which dynamically defined during
the inventorization phase.

Also it is possible to pass extra variables via cli using standard flag **-e**,
like this `-e '{"apt_update":true, "health_check":false}'`.

[gather_customizations.yml](../playbooks/gather_customizations.yml)
-------------------------------------------------------------------
Makes sure that customizations were not gathered already and then gathers them.
If you need to gather it again you can use flag **gather_customizations**.

Runs the set of tasks based on set of flags which allow or deny executing some
tasks. Uses
[vars/steps/gather_customizations.yml](../playbooks/vars/steps/gather_customizations.yml)
set of flags.

Runs the following tasks:
* [health_checks.yml](#health_checksyml)
* [apt_update.yml](#apt_updateyml)
* [get_current_mu.yml](#get_current_muyml)
* [gather_customizations.yml](#clean_customizationsyml)

[verify_patches.yml](../playbooks/verify_patches.yml)
-----------------------------------------------------
Verify customizations by  verify applying patches on target version of packages
**pkg_ver_for_verification**.

Uses [vars/steps/verify_patches.yml](../playbooks/vars/steps/verify_patches.yml)
set of flags.

Runs only two steps:
* [apt_update.yml](#apt_updateyml)
* [verify_customizations.yml](#verify_customizationsyml)
* [verify_patches.yml](#verify_patchesyml)

[update_fuel.yml](../playbooks/update_fuel)
-------------------------------------------
Update the Master node itself.

Runs the following tasks:
* Stop containers
* Make a backup of DB and config files
* Yum update
* Load new images
* Rebuild and start containers

[apply_mu.yml](../playbooks/apply_mu.yml)
-----------------------------------------
Apply MU, apply patches and re-apply current customizations(if enabled).

By default uses [var/steps/apply_mu.yml](../playbooks/vars/steps/apply_mu.yml)
set of flags.

Runs the following tasks:
* [health_checks.yml](#health_checksyml)
* [apt_update.yml](#apt_updateyml)
* [get_current_mu.yml](#get_current_muyml)
* [gather_customizations.yml](#gather_customizationsyml)
* [verify_customizations.yml](#verify_customizationsyml)
* [verify_patches.yml](#verify_patchesyml)
* [apt_upgrade.yml](#apt_upgradeyml)
* [apply_patches.yml](#apply_patchesyml)

and then include one more playbook:
* [restart_services.yml](#restart_servicesyml)

[restart_services.yml](../playbooks/restart_services.yml)
---------------------------------------------------------
Restart all services for each role specified in
[vars/mos_releases/{{ mos_releases }}.yml](../playbooks/vars/mos_releases).

Might be used separately.

[rollback.yml](../playbooks/rollback.yml)
-----------------------------------------
This is a pseudo rollback, since it does not save the current state, but provide
you a mechanism for installing any MU release (that you have initially for
rollback) and apply gathered customizations, of course as usual with verifying
patches before installing.

Runs the following tasks:
* [health_checks.yml](#health_checksyml)
* [apt_update.yml](#apt_updateyml)
* [verify_md5.yml](#verify_md5yml)
* [gather_customizations.yml](#gather_customizationsyml)
* [verify_customizations.yml](#verify_customizationsyml)
* [verify_patches.yml](#verify_patchesyml)
* [rollback_upgrade.yml](#rollback_upgradeyml)
* [apply_patches.yml](#apply_patchesyml)

and then include one more playbook:
* [restart_services.yml](#restart_servicesyml)

Uses [vars/steps/rollback.yml](../playbooks/vars/steps/rollback.yml) set of
flags.

[update_ceph.yml](../playbooks/update_ceph.yml)
-----------------------------------------
This is a separate playbook that uses some ideas and solutions introduced in
ceph-ansible project to update ceph installations deployed by Fuel.

Runs the following tasks:
* [health_checks.yml](#health_checksyml)
* [update_ceph.yml](#tasksupdate_cephyml)
* [restart_ceph.yml](#restart_cephyml)

MON, OSD and RGW nodes are updated one-by-one. The following features are used
to protect data and achieve zero downtime:
* Quorum is checked after MONs services restart to ensure that updated host
came up and running.
* scrub and deep-scrub operations are stopped during OSDs update.
* noout flag is used to activate maintenance mode for a cluster.
* Clusters health is checked to ensure that updated OSD came up and running.
* Any failure during any update play will stop the whole update process.

This playbook should be runned with env_id specified using standard flag **-e**.
For example: `-e '{"env_id":1}'`.

Uses [vars/steps/update_ceph.yml](../playbooks/vars/steps/apply_mu.yml) set
of flags.


Tasks
=====

[health_checks.yml](../playbooks/tasks/health_checks.yml)
-------------------------------------------------------
* Check health of Fuel [health_check_fuel.sh](../playbooks/files/health_check_fuel.sh)
* Check health of OpenStack services [health_check_os.sh](../playbooks/files/health_check_os.sh)
* Check free space on nodes
* Check status of Pacemaker resources

[apt_update.yml](../playbooks/tasks/apt_update.yml)
---------------------------------------------------
* Clean **apt** folder on nodes if **apt_update** flag is true
* Check if **apt** folder already exists, if exists skip generation process
* If **apt** dir does not exist performed
  [apt_update_action.yml](#apt_update_actionyml)

[apt_update_action.yml](../playbooks/tasks/apt_update_action.yml)
-----------------------------------------------------------------
* Generate and copy on nodes sources.list files from
  [templates/sources.list.j2](../playbooks/templates/sources.list.j2) using
  configured repositories in conf file.
* Generate and copy on nodes **apt.conf** file from
  [templates/apt_conf.j2](../playbooks/templates/apt.conf.j2).
* Perform APT update using generated **apt.conf** on nodes.

[verify_md5.yml](../playbooks/tasks/verify_md5.yml)
---------------------------------------------------
* Runs [files/verify_md5_packages_ubuntu.sh](../playbooks/files/verify_md5_packages_ubuntu.sh)
  script which for all installed packages calculates MD5 sum and compares
  with origin.
* Return list of customized packages in **md5_verify_result** variable.

[get_current_mu.yml](../playbooks/tasks/get_current_mu.yml)
-----------------------------------------------------------
* Runs [files/get_current_mu.sh](../playbooks/files/get_current_mu.sh) script to
  identify which MU currently is applied. Actually this script just uses one by
  one sources.list from sources.list.d folder and check if any package have
  available 'update'. If no one have update it means that exactly this MU is
  installed now. It can return 'undefined' result, that means the node has
  installed packages from different MU(or other undefined) repos.

[gather_customizations.yml](../playbooks/tasks/gather_customizations.yml)
-------------------------------------------------------------------------
* include [verify_md5.yml](#verify_md5yml)
* include [get_current_mu.yml](#get_current_muyml)
* Make sure **patches** directory exists on Fuel
* Check **gather_customizations** flag, if enabled clean customizations
  folder on Fuel and nodes
* Check if customizations are gathered ( **customizations** folder exists on Fuel)
* If does not exist include [gather_customizations_action.yml](#gather_customizations_actionyml)

[gather_customizations_action.yml](../playbooks/tasks/gather_customizations_action.yml)
---------------------------------------------------------------------------------------
* For each customized package in **md5_verify_result** run
  [files/get_package_customizations.sh](../playbooks/files/get_package_customizations.sh).
  This script unpacks cached origin installed package (or download if cached does
  not exist) and make a diff between origin and current customized file.
* Please be aware if package was customized by **adding new files it will NOT
  be detected !!! **
* If customizations were gathered, download them on Fuel in
  **customizations/\<env_id\>/\<nodename\>**.

[verify_customizations.yml](../playbooks/tasks/verify_customizations.yml)
-------------------------------------------------------------------------
This task checks the consistency of gathered custom patches across the environment.
Actually consistency is checked by the following way:
- For each customized package create folder
  **customizations_verification/\<packagename\>**
- All customizations for this package will be copied to
  **customizations_verification/\<packagename\>/\<nodename\>**
- For each customization for specific package calculates MD5 Sum

Then we can get 3 situation:
  1. All nodes have the same customization
  2. Some nodes have the same customization and the rest do not have any customization
  3. Some nodes have different customizations

Results for these cases:
  1. Copy this customization to **patches/00-customizations** folder
  2. Fail and print an instruction with flag **unify_only_patches**,
which allows to ignore this warning and continue
  3. Fail and print an instruction how to fix this issue.

Runs the following tasks:
* Check whether flag **use_current_customization** is enabled
* Run script [verify_customizations_consistency.sh](../playbooks/files/verify_customizations_consistency.sh)
* Copy unified customizations to **patches/00-customizations** folder

[verify_patches.yml](../playbooks/tasks/verify_patches.yml)
-----------------------------------------------------------
* Clean **patches** folder on nodes.
* Clean **verification** folder on nodes.
* Copy patches from Fuel folder **patches** to nodes folder **patches**
  (if **rollback** is not enabled).
* Run [files/rollback_customizations.sh](../playbooks/files/rollback_customizations.sh)
  script which copy current patches from **customizations** folder to
  **patches** folder (if **use_current_customization** is enabled).
* Run [files/verify_patches.sh](../playbooks/files/verify_patches.sh) script
  which:
    * Make sure that each patch affects only one package.
    * Download and extract candidate package if it does not already exist.
    * Try to apply patch. If more than 1 patch affects this package they will
      be applied in alphabetic order.

[apt_upgrade.yml](../playbooks/tasks/apt_upgrade.yml)
-----------------------------------------------------
* Correct dependencies.
* Perform APT upgrade.
* Reinstall customized packages

[apply_patches.yml](../playbooks/tasks/apply_patches.yml)
---------------------------------------------------------
* Run [files/apply_patches.sh](../playbooks/files/apply_patches.sh) script
  which just applies sorted by relative name patches in **patches** folder on
  nodes.

[rollback_upgrade.yml](../playbooks/tasks/rollback_upgrade.yml)
---------------------------------------------------------------
* Correct dependencies.
* Perform APT upgrade using only specified in variable **rollback** MU name.
* Reinstall customized packages

[tasks/update_ceph.yml](../playbooks/tasks/update_ceph.yml)
---------------------------------------------------------------
* Gather a list of installed ceph packages.
* Update all listed packages one-by-one.

[restart_ceph.yml](../playbooks/tasks/restart_ceph.yml)
---------------------------------------------------------------
* Restart specific ceph services.
* Ensure that restart finished correctly and services are up and running

